---
title: Project Teddy - YOLO + LSTM To Predict My Brother's Dog's Behavior
description: A Hybrid approach to predicting my brothers dog's behavior.
date: 12-29-2024
updated_at: 12-29-2024
show_table_of_contents: true
pinned: true
---

## Introduction

Creating an accurate prediction model doesn't always require heavy, resource-intensive techniques. This project is a prototype that leverages lightweight methods to achieve this goal. To achieve this, this prototype uses object detection [YOLO](https://github.com/ultralytics/ultralytics) and sequential model [LSTM](https://github.com/keras-team/keras/) to use sequential data to make predictions.

One of my primary objectives was to experiment with techniques to minimize inference time on my CPU. For instance, when observing Teddy lying down, I decided to skip inferencing every two frames since his state was unlikely to change rapidly. Other methods can include [conditional computing](https://intellabs.github.io/distiller/conditional_computation.html), [pruning](https://intellabs.github.io/distiller/pruning.html), [Quantization](https://intellabs.github.io/distiller/quantization.html), and more. I would love to add these in the future.

I haven't come across many projects similar to this one, but I did find In fact, the founder of Ultralytics recently complimented this approach in [this discussion](https://github.com/ultralytics/ultralytics/issues/8496), highlighting its potential.

<YouTubeVideo videoid="gNqRSZjK1wQ"/>

### Use Cases

You might find this project particularly interesting if you are working with:

- **Drones, edge devices, and other devices with limited compute**: These platforms benefit greatly from lightweight models that don't demand excessive computational resources.
- **Applications that need to handle multiple input streams**: Reducing inference time is key if you need a real time application consuming multiple input streams.
- **Optimizing latency by small local models**: Reducing latency is crucial for real-time applications, and local models can help achieve that.

### Some Common Approaches

When it comes to object detection and image classification, models generally fall into two categories: *Lightweight* and **Heavyweight**.

#### Lightweight Models

**YOLO** is a standout in this category, offering extremely fast object detection while maintaining high accuracy. It's widely used in real-time applications where speed is paramount.

**SSD** is another popular model similar to YOLO. It's one of YOLO's main competitors and is known for its balance between speed and accuracy.

**CNNs** classify images into single labels efficiently.

#### Heavyweight Models

On the other end of the spectrum, **DETR**is a transformer-based model used for object detection. Transformers excel at capturing long-range dependencies in data, which can enhance detection accuracy but at the cost of increased computational resources.

**ViT** is another transformer-based model designed for image classification. Like DETR, it offers high accuracy but requires more compute power and longer inference times.

These heavier models are highly accurate, but they demand more computational resources and longer inference times. There's a continuous wave of research aimed at making these models faster without sacrificing accuracy. It feels like there's a new paper every week!

Here is a cool multimodal [project](https://www.mdpi.com/2079-9292/10/9/1035) that showcases YOLO detecting signs from sign language and using an LSTM to predict the corresponding words. Quite similar to this one.

## The Vision

<Image
  alt={`Flow Diagram`}
  src={`/other/multimodal-flow-diagram.png`}
  width={600}
  height={350}
/>

In this project, I decided to combine the strengths of YOLO for visual feature extraction with LSTM networks for handling sequential data. This combination allows the model to predict the probabilities of the next state of my brother's dog, Teddy.

### Classes

The model identifies the following classes:

- **teddy**: Teddy's normal state.
- **teddy_lying**: Teddy is lying down.
- **teddy_playing**: Teddy is playing with one of his toys.
- **teddy_howling**: Teddy is howling.
- **toy**
- **water bowl**
- **food bowl**

By extracting the bounding box coordinates (x, y, w, h) and their respective changes (dx, dy) from the previous frame, I feed this data into the LSTM. This setup has achieved satisfactory accuracy.

**Input to the LSTM:**

| frame_idx | teddy_state | teddy_confidence | teddy_x | teddy_y | teddy_w | teddy_h | teddy_dx | teddy_dy | closest_toy_confidence | closest_toy_x | closest_toy_y | closest_toy_w | closest_toy_h | closest_toy_dx | closest_toy_dy | water_confidence | water_x | water_y | water_w | water_h |
|-----------|-------------|-------------------|---------|---------|---------|---------|----------|----------|------------------------|---------------|---------------|---------------|---------------|----------------|----------------|------------------|---------|---------|---------|---------|
```python
{
'frame_idx': int,
'teddy_state': str,
'teddy_confidence': float,
'teddy_x': float,
'teddy_y': float,
'teddy_w': float,
'teddy_h': float,
'teddy_dx': float,
'teddy_dy': float,
'closest_toy_confidence': float,
'closest_toy_x': float,
'closest_toy_y': float,
'closest_toy_w': float,
'closest_toy_h': float,
'closest_toy_dx': float,
'closest_toy_dy': float,
'water_confidence': float,
'water_x': float,
'water_y': float,
'water_w': float,
'water_h': float
}
```

**Output looks something like:**

```python
{
'frame_idx': int,
'current_state': str,
'predicted_states': state1: float, state2: float...
'predicted_next_state': str
}
```

### Challenges

Throughout this project, I encountered several challenges:

- **Underfitted Classes**: Classes like `teddy_howling` had insufficient data.
- **Camera Stability**: Shaky cameras or moving backgrounds caused inaccuracies.
- **Multiple States**: Teddy sometimes exhibited behaviors that fit into two states simultaneously, complicating the classification by object detection. It was hard to even label it in the training data.

### Tips & Tricks

Here are some insights and strategies that helped overcome the challenges:

- **Still Background Class**: Initially, I considered adding a fifth state, such as `teddy_drinking` or `teddy_eating`. However, I realized there wasn't enough data for these classes. Instead, maintaining a still object still proved beneficial for the LSTM, as any movement would indicate camera shake, reducing the weight of the `dx` and `dy` parameters.
- **Skipping Inference**: By identifying when Teddy was in a "steady state" like `teddy_lying`, I chose to skip inferencing, which optimized performance.
- **Prioritizing States**: Assigning priorities to Teddy's statesâ€”`teddy_howling` > `teddy_playing` > `teddy_lying` > `teddy` (normal) assisted in addressing underfitting and resolving instances where Teddy was in multiple states simultaneously.
- **(TODO) Audio**: I envisioned adding audio inputs to help detect states like `teddy_howling` and `teddy_playing`. While this would have made the project truly multimodal, the lack of audio in the training data would be a huge lift.

## Future Improvements

Looking ahead, there are several enhancements I'd like to explore:

- **Audio**: Would help increase the accuracy of the model. It would also be fun to experiment with. For example I can use conditional computing to see if i get better performance.
- **Tiny Model Suite**: I'm interested to see how much better performance I can get by using a tiny YOLO model and how accurate it is.
- **Hardware Optimization**: Interested in seeing difference in inference speeds on a gpu versus a cpu. They handle different types of tasks differently.